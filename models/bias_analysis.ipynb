{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'no_protected_model'  # Folder in results containing data and where analysis will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.load(f'results/{folder}/pred.npy')\n",
    "testX = np.load(f'results/{folder}/testX.npy')\n",
    "testY = np.load(f'results/{folder}/testY.npy')\n",
    "print(f\"Shapes: {preds.shape}, {testX.shape}, {testY.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import partially preprocessed version so there aren't too many columns\n",
    "test_df = pd.read_csv('dataset/test_split_partially_preprocessed.csv')\n",
    "# test_df = pd.read_csv('dataset/test_preprocessed.csv')  # Full dataset\n",
    "\n",
    "# Add truncation (since this wasn't done for partially_preprocessed version)\n",
    "test_df.loc[test_df.INCWAGE_CPIU_2010 > 100000, 'INCWAGE_CPIU_2010'] = 100000\n",
    "\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testY[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(testY).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ensures that the indices are correct\n",
    "assert((test_df.INCWAGE_CPIU_2010 == pd.Series(testY)).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if SERIAL or PERNUM are in testX\n",
    "# for i in range(testX.shape[1]):\n",
    "#     seriesConverted = pd.Series(testX[:,i])\n",
    "#     if (test_df.SERIAL == seriesConverted).all():\n",
    "#         print(f\"Column {i} is SERIAL\")\n",
    "#     if (test_df.PERNUM == seriesConverted).all():\n",
    "#         print(f\"Column {i} is PERNUM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Old shape: \", test_df.shape)\n",
    "test_df['Income_Pred'] = preds\n",
    "print(\"New shape: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are metrics with no rounding\n",
    "test_df['Income_Pred'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Income_Pred'] = test_df['Income_Pred'].round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are metrics with rounding\n",
    "test_df['Income_Pred'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.INCWAGE_CPIU_2010.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for biases"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create underprediction/overprediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of predictions that are perfect\n",
    "(test_df.Income_Pred.round(0) == test_df.INCWAGE_CPIU_2010).values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Pred_Error'] = test_df['Income_Pred'] - test_df['INCWAGE_CPIU_2010']\n",
    "test_df['Pred_Error'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Pred_AbsError'] = test_df['Pred_Error'].abs()\n",
    "test_df['Pred_AbsError'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Income summary\")\n",
    "print(test_df.INCWAGE_CPIU_2010.describe())\n",
    "print(\"\\nIncome prediction summary\")\n",
    "print(test_df.Income_Pred.describe())\n",
    "print(\"\\nAbsolute error summary\")\n",
    "print(test_df.Pred_AbsError.describe())\n",
    "print(\"\\nRelative error summary\")\n",
    "print(test_df.Pred_Error.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test differences in accuracy and under/overprediction rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_cols = [\n",
    "    'isFemale', \n",
    "    'isAmericanIndian', 'isAsian', 'isBlack', 'isPacificIslander', 'isWhite', 'isOtherRace', 'isHispanic',\n",
    "    'bornInUS',\n",
    "    'isMarried', 'wasMarried', 'neverMarried',\n",
    "    'sameSexMarriage', 'mixedRaceMarriage',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalRows = len(test_df)  # Total number of rows in the dataset\n",
    "summaryEntries = 0  # Number of entries in the summary\n",
    "\n",
    "with open(f'results/{folder}/analysis/summary1.txt', 'w') as f:\n",
    "    for col in protected_cols:\n",
    "        if test_df[col].dtype != 'bool':\n",
    "            raise Exception(f\"Column {col} is not boolean\")\n",
    "\n",
    "        assert(not test_df[col].isna().values.any())\n",
    "\n",
    "        numTrue = test_df[col].values.sum()  # Number of entries for which the column is true\n",
    "        numFalse = totalRows - numTrue  # Number of entries for which the column is false\n",
    "\n",
    "        dfTrue = test_df[test_df[col]]\n",
    "        dfFalse = test_df[~test_df[col]]\n",
    "\n",
    "        print(f'Of the people for whom {col} is true ({numTrue} of {totalRows} entries, or {numTrue / totalRows * 100}%), actual salaries are:')\n",
    "        print(dfTrue.INCWAGE_CPIU_2010.describe())\n",
    "        print(\"Predictions are\")\n",
    "        print(dfTrue.Income_Pred.describe())\n",
    "        print(\"Absolute error is:\")\n",
    "        print(dfTrue.Pred_AbsError.describe())\n",
    "        print(\"Relative error is:\")\n",
    "        print(dfTrue.Pred_Error.describe())\n",
    "        print(\"\\n\")\n",
    "        print(f'Of the people for whom {col} is false ({numFalse} of {totalRows} entries, or {numFalse / totalRows * 100}%), actual salaries are:')\n",
    "        print(dfFalse.INCWAGE_CPIU_2010.describe())\n",
    "        print(\"Predictions are\")\n",
    "        print(dfFalse.Income_Pred.describe())\n",
    "        print(\"Absolute error is:\")\n",
    "        print(dfFalse.Pred_AbsError.describe())\n",
    "        print(\"Relative error is:\")\n",
    "        print(dfFalse.Pred_Error.describe())\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        # Look for interesting cases to add to summary\n",
    "\n",
    "        # Large difference in mean prediction\n",
    "        if(abs(dfTrue.Income_Pred.mean() - dfFalse.Income_Pred.mean()) > 1000):\n",
    "            f.write(f\"Mean prediction for {col} is significantly different than mean prediction for not {col}\\n\")\n",
    "            f.write(f\"Mean prediction for {col}: {dfTrue.Income_Pred.mean()}\")\n",
    "            f.write(f\"\\tMean prediction for not {col}: {dfFalse.Income_Pred.mean()}\")\n",
    "            f.write(\"\\n\\n\")\n",
    "            summaryEntries += 1\n",
    "        # Large difference in mean absolute error\n",
    "        if(abs(dfTrue.Pred_AbsError.mean() - dfFalse.Pred_AbsError.mean()) > 1000):\n",
    "            f.write(f\"Mean absolute error for {col} is significantly different than mean absolute error for not {col}\\n\")\n",
    "            f.write(f\"Mean absolute error for {col}: {dfTrue.Pred_AbsError.mean()}\")\n",
    "            f.write(f\"\\tMean absolute error for not {col}: {dfFalse.Pred_AbsError.mean()}\")\n",
    "            f.write(\"\\n\\n\")\n",
    "            summaryEntries += 1\n",
    "        # Large difference in mean relative error\n",
    "        if(abs(dfTrue.Pred_Error.mean() - dfFalse.Pred_Error.mean()) > 750):\n",
    "            f.write(f\"Mean relative error for {col} is significantly different than mean relative error for not {col}\\n\")\n",
    "            f.write(f\"Mean relative error for {col}: {dfTrue.Pred_Error.mean()}\")\n",
    "            f.write(f\"\\tMean relative error for not {col}: {dfFalse.Pred_Error.mean()}\")\n",
    "            f.write(\"\\n\\n\")\n",
    "            summaryEntries += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryEntries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full analysis (this includes every column, not just the ones that are considered protected). I'd recommend either only using the summary or using this in conjunction with the shap analysis since the detailed text file is so large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalRows = len(test_df)  # Total number of rows in the dataset\n",
    "summaryEntries = 0  # Number of entries in the summary\n",
    "\n",
    "with open(f'results/{folder}/analysis/summary2.txt', 'w') as f:\n",
    "    for col in test_df.columns:\n",
    "        if test_df[col].dtype != 'bool':\n",
    "            continue\n",
    "            # raise Exception(f\"Column {col} is not boolean\")\n",
    "\n",
    "        assert(not test_df[col].isna().values.any())\n",
    "\n",
    "        numTrue = test_df[col].values.sum()  # Number of entries for which the column is true\n",
    "        numFalse = totalRows - numTrue  # Number of entries for which the column is false\n",
    "\n",
    "        dfTrue = test_df[test_df[col]]\n",
    "        dfFalse = test_df[~test_df[col]]\n",
    "\n",
    "        print(f'Of the people for whom {col} is true ({numTrue} of {totalRows} entries, or {numTrue / totalRows * 100}%), actual salaries are:')\n",
    "        print(dfTrue.INCWAGE_CPIU_2010.describe())\n",
    "        print(\"Predictions are\")\n",
    "        print(dfTrue.Income_Pred.describe())\n",
    "        print(\"Absolute error is:\")\n",
    "        print(dfTrue.Pred_AbsError.describe())\n",
    "        print(\"Relative error is:\")\n",
    "        print(dfTrue.Pred_Error.describe())\n",
    "        print(\"\\n\")\n",
    "        print(f'Of the people for whom {col} is false ({numFalse} of {totalRows} entries, or {numFalse / totalRows * 100}%), actual salaries are:')\n",
    "        print(dfFalse.INCWAGE_CPIU_2010.describe())\n",
    "        print(\"Predictions are\")\n",
    "        print(dfFalse.Income_Pred.describe())\n",
    "        print(\"Absolute error is:\")\n",
    "        print(dfFalse.Pred_AbsError.describe())\n",
    "        print(\"Relative error is:\")\n",
    "        print(dfFalse.Pred_Error.describe())\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        # Look for interesting cases to add to summary\n",
    "\n",
    "        # Large difference in mean prediction\n",
    "        if(abs(dfTrue.Income_Pred.mean() - dfFalse.Income_Pred.mean()) > 3000):\n",
    "            f.write(f\"Mean prediction for {col} is significantly different than mean prediction for not {col}\\n\")\n",
    "            f.write(f\"Mean prediction for {col}: {dfTrue.Income_Pred.mean()}\")\n",
    "            f.write(f\"\\tMean prediction for not {col}: {dfFalse.Income_Pred.mean()}\")\n",
    "            f.write(\"\\n\\n\")\n",
    "            summaryEntries += 1\n",
    "        # Large difference in mean absolute error\n",
    "        if(abs(dfTrue.Pred_AbsError.mean() - dfFalse.Pred_AbsError.mean()) > 2000):\n",
    "            f.write(f\"Mean absolute error for {col} is significantly different than mean absolute error for not {col}\\n\")\n",
    "            f.write(f\"Mean absolute error for {col}: {dfTrue.Pred_AbsError.mean()}\")\n",
    "            f.write(f\"\\tMean absolute error for not {col}: {dfFalse.Pred_AbsError.mean()}\")\n",
    "            f.write(\"\\n\\n\")\n",
    "            summaryEntries += 1\n",
    "        # Large difference in mean relative error\n",
    "        if(abs(dfTrue.Pred_Error.mean() - dfFalse.Pred_Error.mean()) > 1500):\n",
    "            f.write(f\"Mean relative error for {col} is significantly different than mean relative error for not {col}\\n\")\n",
    "            f.write(f\"Mean relative error for {col}: {dfTrue.Pred_Error.mean()}\")\n",
    "            f.write(f\"\\tMean relative error for not {col}: {dfFalse.Pred_Error.mean()}\")\n",
    "            f.write(\"\\n\\n\")\n",
    "            summaryEntries += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryEntries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
