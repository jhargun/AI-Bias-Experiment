{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "train_df = pd.read_csv('dataset/train_split_partially_preprocessed.csv')\n",
    "test_df = pd.read_csv('dataset/test_split_partially_preprocessed.csv')\n",
    "print(\"Train: \", train_df.shape, \"\\tTest: \", test_df.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create state fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_nostate = (df.PWSTATE2 == 0).values.sum()\n",
    "# original_len = len(df)\n",
    "# print(f\"{num_nostate} ({num_nostate/original_len:.2%}) of the rows have no state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove rows with no state\n",
    "# df = df[df.PWSTATE2 != 0]\n",
    "# assert(len(df) == original_len - num_nostate)\n",
    "# print(f\"Removed {num_nostate} rows with no state. {len(df)} rows remain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop columns that are now unnecessary\n",
    "\n",
    "# origNumCols = len(df.columns)\n",
    "# droppedCols = []\n",
    "\n",
    "# for col in df.columns:\n",
    "#     unique = df[col].unique()\n",
    "#     if(len(unique) == 1):\n",
    "#         print(f\"Dropping column {col} since it has only one value: {unique[0]}\")\n",
    "#         droppedCols.append(col)\n",
    "#     elif(len(unique) == 2 and df[col].isna().values.any()):\n",
    "#         print(f\"Warning: Column {col} has two values but you may still want to drop it: {unique[0]} and {unique[1]}\")\n",
    "\n",
    "# df.drop(droppedCols, axis=1, inplace=True)\n",
    "# assert(len(df.columns) == origNumCols - len(droppedCols))\n",
    "# print(f\"Dropped {len(droppedCols)} columns. {len(df.columns)} columns remain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(open('ipums_fields/stateField.json')) as f:\n",
    "    state_mapping = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure each degree field is in the mapping\n",
    "stateKeys = sorted([int(k) for k in state_mapping.keys() if int(k) != 0])\n",
    "# Note that stateKeys doesn't include foreign countries\n",
    "assert(sorted([val for val in train_df.PWSTATE2.unique().tolist() if val <= stateKeys[-1]]) == stateKeys)\n",
    "assert(sorted([val for val in test_df.PWSTATE2.unique().tolist() if val <= stateKeys[-1]]) == stateKeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"worksOutsideUS\"] = (train_df.PWSTATE2 > stateKeys[-1])\n",
    "test_df[\"worksOutsideUS\"] = (test_df.PWSTATE2 > stateKeys[-1])\n",
    "\n",
    "cols_created = 1\n",
    "for (key, value) in state_mapping.items():\n",
    "    # Skip N/A column (may want to fill this with NaN later)\n",
    "    if value == 'N/A':\n",
    "        continue\n",
    "\n",
    "    stateName = value.replace(' ', '_')\n",
    "    train_df[f\"worksIn_{stateName}\"] = (train_df.PWSTATE2 == int(key))\n",
    "    test_df[f\"worksIn_{stateName}\"] = (test_df.PWSTATE2 == int(key))\n",
    "    cols_created += 1\n",
    "\n",
    "print(f\"Created {cols_created} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=['PWSTATE2', 'PWCOUNTY', 'PWTYPE'], inplace=True)\n",
    "test_df.drop(columns=['PWSTATE2', 'PWCOUNTY', 'PWTYPE'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create degree fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(open('ipums_fields/degField.json')) as f:\n",
    "    deg_mapping = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure each degree field is in the mapping\n",
    "assert(sorted(train_df.DEGFIELD.unique().tolist()) == sorted([int(k) for k in deg_mapping.keys()]))\n",
    "assert(sorted(test_df.DEGFIELD.unique().tolist()) == sorted([int(k) for k in deg_mapping.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_created = 0\n",
    "for (key, value) in deg_mapping.items():\n",
    "    # Skip N/A column (may want to fill this with NaN later)\n",
    "    if value == 'N/A':\n",
    "        continue\n",
    "\n",
    "    degName = value.replace(' ', '_').replace(',', '')\n",
    "    train_df[f\"hasDegree_{degName}\"] = (train_df.DEGFIELD == int(key))\n",
    "    test_df[f\"hasDegree_{degName}\"] = (test_df.DEGFIELD == int(key))\n",
    "    cols_created += 1\n",
    "\n",
    "print(f\"Created {cols_created} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=['DEGFIELD', 'DEGFIELDD'], inplace=True)\n",
    "test_df.drop(columns=['DEGFIELD', 'DEGFIELDD'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create occupation fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(open('ipums_fields/occupation2010.json')) as f:\n",
    "    occ_mapping = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(sorted(train_df.OCC2010.unique().tolist()) == sorted(test_df.OCC2010.unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure each degree field is in the mapping\n",
    "sortedOcc = sorted([int(k) for k in occ_mapping.keys()])\n",
    "uniqueCodes = train_df.OCC2010.unique().tolist()\n",
    "removedKeys = []\n",
    "\n",
    "for jobCode in uniqueCodes:\n",
    "    assert(jobCode in sortedOcc)\n",
    "\n",
    "for jobCode in sortedOcc:\n",
    "    if jobCode not in uniqueCodes:\n",
    "        removedKeys.append(jobCode)\n",
    "        print(f\"No examples of job code {jobCode} (occupation {occ_mapping[str(jobCode)]})\")\n",
    "# assert(sorted(df.OCC2010.unique().tolist()) == )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_created = 0\n",
    "for (key, value) in occ_mapping.items():\n",
    "    # Skip N/A column (may want to fill this with NaN later)\n",
    "    if value == 'N/A':\n",
    "        continue\n",
    "    \n",
    "    # Skip occupations that were removed\n",
    "    if key in removedKeys:\n",
    "        continue\n",
    "\n",
    "    occName = value.replace(' ', '_').replace(',', '')\n",
    "    train_df[f\"occupation_{occName}\"] = (train_df.OCC2010 == int(key))\n",
    "    test_df[f\"occupation_{occName}\"] = (test_df.OCC2010 == int(key))\n",
    "    cols_created += 1\n",
    "\n",
    "print(f\"Created {cols_created} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Created {cols_created} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=['OCC2010'], inplace=True)\n",
    "test_df.drop(columns=['OCC2010'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.copy()\n",
    "test_df = test_df.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index().to_csv('dataset/train_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.reset_index().to_csv('dataset/test_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a60c63dd419133ec943074489a186d09dce25069fc0d4ed86e3ad3c69baefe89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
