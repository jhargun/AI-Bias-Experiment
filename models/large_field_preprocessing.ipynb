{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "train_df = pd.read_csv('dataset/train_split_partially_preprocessed.csv')\n",
    "test_df = pd.read_csv('dataset/test_split_partially_preprocessed.csv')\n",
    "print(\"Train: \", train_df.shape, \"\\tTest: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = pd.read_csv('dataset/test_split_partially_preprocessed.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create state fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_nostate = (df.PWSTATE2 == 0).values.sum()\n",
    "# original_len = len(df)\n",
    "# print(f\"{num_nostate} ({num_nostate/original_len:.2%}) of the rows have no state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove rows with no state\n",
    "# df = df[df.PWSTATE2 != 0]\n",
    "# assert(len(df) == original_len - num_nostate)\n",
    "# print(f\"Removed {num_nostate} rows with no state. {len(df)} rows remain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop columns that are now unnecessary\n",
    "\n",
    "# origNumCols = len(df.columns)\n",
    "# droppedCols = []\n",
    "\n",
    "# for col in df.columns:\n",
    "#     unique = df[col].unique()\n",
    "#     if(len(unique) == 1):\n",
    "#         print(f\"Dropping column {col} since it has only one value: {unique[0]}\")\n",
    "#         droppedCols.append(col)\n",
    "#     elif(len(unique) == 2 and df[col].isna().values.any()):\n",
    "#         print(f\"Warning: Column {col} has two values but you may still want to drop it: {unique[0]} and {unique[1]}\")\n",
    "\n",
    "# df.drop(droppedCols, axis=1, inplace=True)\n",
    "# assert(len(df.columns) == origNumCols - len(droppedCols))\n",
    "# print(f\"Dropped {len(droppedCols)} columns. {len(df.columns)} columns remain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(open('ipums_fields/stateField.json')) as f:\n",
    "    state_mapping = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure each degree field is in the mapping\n",
    "stateKeys = sorted([int(k) for k in state_mapping.keys() if int(k) != 0])\n",
    "# Note that stateKeys doesn't include foreign countries\n",
    "assert(sorted([val for val in train_df.PWSTATE2.unique().tolist() if val <= stateKeys[-1]]) == stateKeys)\n",
    "assert(sorted([val for val in test_df.PWSTATE2.unique().tolist() if val <= stateKeys[-1]]) == stateKeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"worksOutsideUS\"] = (train_df.PWSTATE2 > stateKeys[-1])\n",
    "test_df[\"worksOutsideUS\"] = (test_df.PWSTATE2 > stateKeys[-1])\n",
    "\n",
    "cols_created = 1\n",
    "for (key, value) in state_mapping.items():\n",
    "    # Skip N/A column (may want to fill this with NaN later)\n",
    "    if value == 'N/A':\n",
    "        continue\n",
    "\n",
    "    stateName = value.replace(' ', '_')\n",
    "    train_df[f\"worksIn_{stateName}\"] = (train_df.PWSTATE2 == int(key))\n",
    "    test_df[f\"worksIn_{stateName}\"] = (test_df.PWSTATE2 == int(key))\n",
    "    cols_created += 1\n",
    "\n",
    "print(f\"Created {cols_created} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=['PWSTATE2', 'PWCOUNTY', 'PWTYPE'], inplace=True)\n",
    "test_df.drop(columns=['PWSTATE2', 'PWCOUNTY', 'PWTYPE'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create degree fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(open('ipums_fields/degField.json')) as f:\n",
    "    deg_mapping = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure each degree field is in the mapping\n",
    "assert(sorted(train_df.DEGFIELD.unique().tolist()) == sorted([int(k) for k in deg_mapping.keys()]))\n",
    "assert(sorted(test_df.DEGFIELD.unique().tolist()) == sorted([int(k) for k in deg_mapping.keys()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_created = 0\n",
    "for (key, value) in deg_mapping.items():\n",
    "    # Skip N/A column (may want to fill this with NaN later)\n",
    "    if value == 'N/A':\n",
    "        continue\n",
    "\n",
    "    degName = value.replace(' ', '_').replace(',', '')\n",
    "    train_df[f\"hasDegree_{degName}\"] = (train_df.DEGFIELD == int(key))\n",
    "    test_df[f\"hasDegree_{degName}\"] = (test_df.DEGFIELD == int(key))\n",
    "    cols_created += 1\n",
    "\n",
    "print(f\"Created {cols_created} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=['DEGFIELD', 'DEGFIELDD'], inplace=True)\n",
    "test_df.drop(columns=['DEGFIELD', 'DEGFIELDD'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create occupation fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with(open('ipums_fields/occupation2010.json')) as f:\n",
    "    occ_mapping = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(sorted(train_df.OCC2010.unique().tolist()) == sorted(test_df.OCC2010.unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure each degree field is in the mapping\n",
    "sortedOcc = sorted([int(k) for k in occ_mapping.keys()])\n",
    "uniqueCodes = train_df.OCC2010.unique().tolist()\n",
    "removedKeys = []\n",
    "\n",
    "for jobCode in uniqueCodes:\n",
    "    assert(jobCode in sortedOcc)\n",
    "\n",
    "for jobCode in sortedOcc:\n",
    "    if jobCode not in uniqueCodes:\n",
    "        removedKeys.append(jobCode)\n",
    "        print(f\"No examples of job code {jobCode} (occupation {occ_mapping[str(jobCode)]})\")\n",
    "# assert(sorted(df.OCC2010.unique().tolist()) == )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_created = 0\n",
    "for (key, value) in occ_mapping.items():\n",
    "    # Skip N/A column (may want to fill this with NaN later)\n",
    "    if value == 'N/A':\n",
    "        continue\n",
    "    \n",
    "    # Skip occupations that were removed\n",
    "    if key in removedKeys:\n",
    "        continue\n",
    "\n",
    "    occName = value.replace(' ', '_').replace(',', '')\n",
    "    train_df[f\"occupation_{occName}\"] = (train_df.OCC2010 == int(key))\n",
    "    test_df[f\"occupation_{occName}\"] = (test_df.OCC2010 == int(key))\n",
    "    cols_created += 1\n",
    "\n",
    "print(f\"Created {cols_created} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Created {cols_created} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=['OCC2010'], inplace=True)\n",
    "test_df.drop(columns=['OCC2010'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.copy()\n",
    "test_df = test_df.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataframes\n",
    "\n",
    "If you're going to do salary adjustments, don't save dataframes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index().to_csv('dataset/train_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.reset_index().to_csv('dataset/test_preprocessed.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Salary adjustments\n",
    "\n",
    "The following cells contain optional adjustments to salary. Make sure not to run all of them since some are meant to be run alone."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discrete adjustments\n",
    "\n",
    "This creates 11 discrete buckets for classification, with each bucket containing a range of salaries of $10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    col_name = f\"makes{i * 10}To{(i+1) * 10}K\"\n",
    "    train_df[col_name] = (train_df.INCWAGE_CPIU_2010 >= i * 10000) & (train_df.INCWAGE_CPIU_2010 < (i+1) * 10000)\n",
    "    test_df[col_name] = (test_df.INCWAGE_CPIU_2010 >= i * 10000) & (test_df.INCWAGE_CPIU_2010 < (i+1) * 10000)\n",
    "\n",
    "col_name = f\"makesOver100K\"\n",
    "train_df[col_name] = (train_df.INCWAGE_CPIU_2010 >= 100000)\n",
    "test_df[col_name] = (test_df.INCWAGE_CPIU_2010 >= 100000)\n",
    "\n",
    "print(\"Cols created:\", *[col for col in train_df.columns if col.startswith(\"makes\") and train_df[col].dtype == bool and test_df[col].dtype == bool])\n",
    "\n",
    "# train_df.drop(columns=['INCWAGE_CPIU_2010'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(columns=['INCWAGE_CPIU_2010'], inplace=True)\n",
    "test_df.drop(columns=['INCWAGE_CPIU_2010'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index().to_csv('dataset/train_preprocessed_buckets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.reset_index().to_csv('dataset/test_preprocessed_buckets.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also creates 11 discrete buckets for classification. Here, the buckets are created to hold the same number of people. Therefore, some buckets may contain a wider range of values than others.\n",
    "\n",
    "If you run this part, make sure you haven't run the previous 4 cells which create buckets using a different method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion to uint not necessary but makes the file names more readable\n",
    "train_df['INCWAGE_CPIU_2010'] = train_df['INCWAGE_CPIU_2010'].astype('uint')\n",
    "test_df['INCWAGE_CPIU_2010'] = test_df['INCWAGE_CPIU_2010'].astype('uint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.concat([train_df, test_df], ignore_index=True)\n",
    "assert(len(full_df) == len(train_df) + len(test_df))\n",
    "full_df.sort_values(by=['INCWAGE_CPIU_2010'], inplace=True)\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = full_df.reset_index().drop(['index'], axis=1)\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just to make sure my indexing is correct\n",
    "assert((full_df.INCWAGE_CPIU_2010.iloc[len(full_df)-1] == full_df.INCWAGE_CPIU_2010.max()) and (full_df.INCWAGE_CPIU_2010.iloc[0] == full_df.INCWAGE_CPIU_2010.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numBuckets = 11\n",
    "elementsPerBucket = len(full_df) // numBuckets\n",
    "\n",
    "cutoffs = []\n",
    "\n",
    "# Get cutoffs for each bucket\n",
    "for i in range(1, numBuckets):\n",
    "    cutoffs.append(full_df.INCWAGE_CPIU_2010.iloc[i * elementsPerBucket])\n",
    "\n",
    "cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['incomeBucket0'] = (train_df.INCWAGE_CPIU_2010 < cutoffs[0])\n",
    "test_df['incomeBucket0'] = (test_df.INCWAGE_CPIU_2010 < cutoffs[0])\n",
    "\n",
    "for i in range(1, numBuckets - 1):\n",
    "    train_df[f'incomeBucket{i}'] = (train_df.INCWAGE_CPIU_2010 >= cutoffs[i-1]) & (train_df.INCWAGE_CPIU_2010 < cutoffs[i])\n",
    "    test_df[f'incomeBucket{i}'] = (test_df.INCWAGE_CPIU_2010 >= cutoffs[i-1]) & (test_df.INCWAGE_CPIU_2010 < cutoffs[i])\n",
    "\n",
    "train_df[f'incomeBucket{numBuckets - 1}'] = (train_df.INCWAGE_CPIU_2010 >= cutoffs[-1])\n",
    "test_df[f'incomeBucket{numBuckets - 1}'] = (test_df.INCWAGE_CPIU_2010 >= cutoffs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [train_df, test_df]:\n",
    "    # Check that every element is in at least 1 bucket\n",
    "    vals = df.incomeBucket0\n",
    "    for i in range(1, numBuckets):\n",
    "        vals = vals | df[f'incomeBucket{i}']\n",
    "    assert(vals.all())\n",
    "\n",
    "    df.drop(columns=['INCWAGE_CPIU_2010'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index().to_csv(f'dataset/train_variableBuckets_cutoffs{\",\".join([str(c) for c in cutoffs])}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.reset_index().to_csv(f'dataset/test_variableBuckets_cutoffs{\",\".join([str(c) for c in cutoffs])}.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous adjustments\n",
    "These create continuous data for regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide salaries by 5000 to reduce range of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['INCWAGE_CPIU_2010'] = (train_df['INCWAGE_CPIU_2010'].astype(float) / 5000)\n",
    "test_df['INCWAGE_CPIU_2010'] = (test_df['INCWAGE_CPIU_2010'].astype(float) / 5000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cap max salary at 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set ceiling to $100k salary (assumes salaries divided by 5000)\n",
    "train_df.loc[(train_df.INCWAGE_CPIU_2010 > 20), 'INCWAGE_CPIU_2010'] = 20\n",
    "test_df.loc[(test_df.INCWAGE_CPIU_2010 > 20), 'INCWAGE_CPIU_2010'] = 20\n",
    "\n",
    "# Both train and test should have at least 1 example of salaries >= $100k, in which case this assertion should pass\n",
    "assert(train_df.INCWAGE_CPIU_2010.max() == 20 and test_df.INCWAGE_CPIU_2010.max() == 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust range to [-10, 10] (assumes salary capped at 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['INCWAGE_CPIU_2010'] -= 10\n",
    "test_df['INCWAGE_CPIU_2010'] -= 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust salaries to have mean at 0 (ranges will vary depending on data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: mean of train/test should be approximately the same, so could remove the append here\n",
    "meanAdjustment = pd.concat([train_df['INCWAGE_CPIU_2010'], test_df['INCWAGE_CPIU_2010']]).mean()\n",
    "print(f\"Mean is {meanAdjustment}, adjusting train/test data accordingly\")\n",
    "\n",
    "train_df['INCWAGE_CPIU_2010'] -= meanAdjustment\n",
    "test_df['INCWAGE_CPIU_2010'] -= meanAdjustment\n",
    "\n",
    "print(f\"For train data: min is {train_df.INCWAGE_CPIU_2010.min()}, max is {train_df.INCWAGE_CPIU_2010.max()}\")\n",
    "print(f\"For test data: min is {test_df.INCWAGE_CPIU_2010.min()}, max is {test_df.INCWAGE_CPIU_2010.max()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust to normal distribution with mean=0 and std=1. Cutoffs specified below to avoid major outliers. Note that this assumes that only mean adjustment (the cell immediately above this one) was run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the max/min values that will be allowed. Make sure they aren't too small/large since that may include/exclude too many values\n",
    "max_cutoff = 5\n",
    "min_cutoff = -5\n",
    "\n",
    "# Note: std of train/test should be approximately the same, so could remove the append here\n",
    "print(f\"STD of train data is {train_df.INCWAGE_CPIU_2010.std()}. STD of test data is {test_df.INCWAGE_CPIU_2010.std()}. Ensure these values are similar.\")\n",
    "stdAdjustment = pd.concat([train_df['INCWAGE_CPIU_2010'], test_df['INCWAGE_CPIU_2010']]).std()\n",
    "print(f\"STD is {stdAdjustment}, adjusting train/test data accordingly\")\n",
    "\n",
    "train_df['INCWAGE_CPIU_2010'] /= stdAdjustment\n",
    "test_df['INCWAGE_CPIU_2010'] /= stdAdjustment\n",
    "\n",
    "print(f\"For train data: min is {train_df.INCWAGE_CPIU_2010.min()}, max is {train_df.INCWAGE_CPIU_2010.max()}\")\n",
    "print(f\"For test data: min is {test_df.INCWAGE_CPIU_2010.min()}, max is {test_df.INCWAGE_CPIU_2010.max()}\")\n",
    "\n",
    "train_df.loc[(train_df.INCWAGE_CPIU_2010 > max_cutoff), 'INCWAGE_CPIU_2010'] = max_cutoff\n",
    "test_df.loc[(test_df.INCWAGE_CPIU_2010 > max_cutoff), 'INCWAGE_CPIU_2010'] = max_cutoff\n",
    "\n",
    "train_df.loc[(train_df.INCWAGE_CPIU_2010 < min_cutoff), 'INCWAGE_CPIU_2010'] = min_cutoff\n",
    "test_df.loc[(test_df.INCWAGE_CPIU_2010 < min_cutoff), 'INCWAGE_CPIU_2010'] = min_cutoff\n",
    "\n",
    "print(f\"For train data: adjusted min is {train_df.INCWAGE_CPIU_2010.min()}, adjusted max is {train_df.INCWAGE_CPIU_2010.max()}\")\n",
    "print(f\"For test data: adjusted min is {test_df.INCWAGE_CPIU_2010.min()}, adjusted max is {test_df.INCWAGE_CPIU_2010.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'dataset/train_preprocessed_divby5k_meanAdjustedBy{meanAdjustment}_stdAdjustedBy{stdAdjustment}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index().to_csv(f'dataset/train_preprocessed_div5k_cap20_meanAdj{meanAdjustment}_stdAdj{stdAdjustment}_max{train_df.INCWAGE_CPIU_2010.max()}_min{train_df.INCWAGE_CPIU_2010.min()}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.reset_index().to_csv(f'dataset/test_preprocessed_div5k_cap20_meanAdj{meanAdjustment}_stdAdj{stdAdjustment}_max{test_df.INCWAGE_CPIU_2010.max()}_min{test_df.INCWAGE_CPIU_2010.min()}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a60c63dd419133ec943074489a186d09dce25069fc0d4ed86e3ad3c69baefe89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
